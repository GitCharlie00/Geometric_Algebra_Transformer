Geometric Algebra Transformer (GAT) aims to perform deep learning with geometric data (e.g. it performs geometric deep learning).

Geometric data has some properties like the fact that they occupied space, or can rotate, or can flip and so on.
All geometric data are symmetric. This means that if they get a transformation (like rotation and flipping), they don't change their properties (for instance, if we have a molecule and rotate it, then it remains always a molecule).

Classical convolutional neural networks (CNNs) are not able to take into account the symmetry of geometric data. For instance, if we feed an image of a cat to a convolutional layer, then the latter returns a set of feature maps, while if we feed the same image, but rotated, to the same convolutional layer, then the latter returns a set of feature maps that is different from the previous one.
Therefore, we say that classical CNNs are not equivariant (since they don't take into account the symmetry of geometric data).

What we want is to have an architecture that is equivariant and, since tranformers are equivariant, then we can use them to deal with geometric data (and so, create the GAT).
In other words, we want an architecture which is able to take into account that the input image of the cat is the same also if it is rotated (or whatever kind of transformation it got).