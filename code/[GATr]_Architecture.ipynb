{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install \"datasets\" \"pytorch-lightning\" \"wandb\" \"torcheval\" \"torchmetrics\" \"clifford\""
      ],
      "metadata": {
        "id": "NK9Vzh33EHqO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
        "\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from torchmetrics.classification import BinaryPrecision\n",
        "from torchmetrics.classification import BinaryRecall\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "\n",
        "from clifford import Cl\n"
      ],
      "metadata": {
        "id": "lxFbQPFtEGCA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GATr ARCHITECTURE"
      ],
      "metadata": {
        "id": "xd2dhW3y0DwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Claudio's section\n",
        "---"
      ],
      "metadata": {
        "id": "rA54IKoC0H7K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BXAvY8ee0C0K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Jacopo's section\n",
        "---"
      ],
      "metadata": {
        "id": "qtZi3zyB0NlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auxiliar / dummy"
      ],
      "metadata": {
        "id": "bCvJCYeQlnP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xWjorqYhlkrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_mv = {'': 1.0, 'e1': 1.01, 'e2': 1.02, 'e3': 1.03, 'e4': 1.04, 'e12': 2.01, 'e13': 2.02, 'e14': 2.03, 'e23': 2.04, 'e24': 2.05, 'e34': 2.06,  'e123': 3.01, 'e124': 3.02, 'e134': 3.03,  'e234': 3.04, 'e1234': 5.0}"
      ],
      "metadata": {
        "id": "iy8NQYGClWhO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compose_multivector(sample):\n",
        "    wss_multivector = 0\n",
        "    for row in sin_sample['wss'][()]:\n",
        "        vector = row[0] * blades[''] + row[1] * blades[''] + row[2] * blades['']\n",
        "        wss_multivector += vector\n",
        "\n",
        "    # pressure as a pseudoscalar\n",
        "    pressure_multivector = 0\n",
        "    for elem in sin_sample['pressure'][()]:\n",
        "        vector = elem * blades['e1234']\n",
        "        pressure_multivector += vector\n",
        "\n",
        "      # pos as a point\n",
        "    pos_multivector = 0\n",
        "    for row in sin_sample['pos'][()]:\n",
        "        vector = row[0] * blades['e123'] + row[1] * blades['e124'] + row[2] * blades['e134']\n",
        "        pos_multivector += vector\n",
        "\n",
        "      # face as a plane\n",
        "    face_multivector = 0\n",
        "    for row in sin_sample['face'][()]:\n",
        "        vector = row[0] * blades['e1'] + row[1] * blades['e2'] + row[2] * blades['e3']\n",
        "        face_multivector += vector\n",
        "\n",
        "      # inlet as a scalar\n",
        "    inlet_multivector = 0\n",
        "    for elem in sin_sample['inlet_idcs'][()]:\n",
        "        vector = elem * blades['']\n",
        "        inlet_multivector += vector\n",
        "\n",
        "\n",
        "    total_multivector = wss_multivector + pressure_multivector + pos_multivector + face_multivector + inlet_multivector\n",
        "    return total_multivector"
      ],
      "metadata": {
        "id": "hGDT469WNsqD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers"
      ],
      "metadata": {
        "id": "nLVG3A99ljXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class J_norm_linear(pl.LightningModule):\n",
        "    def __init__(self, normalized_shape = (5,), eps=1e-05):\n",
        "        super(J_norm_linear, self).__init__()\n",
        "\n",
        "        self.normalized_shape = normalized_shape # shape of the tesnor\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, multivector):\n",
        "        output = torch.nn.functional.layer_norm( multivector, self.normalized_shape, eps = self.eps )\n",
        "\n",
        "        return output\n",
        "\n",
        "my_layer = J_norm_linear()"
      ],
      "metadata": {
        "id": "LhF_SF8nsZfH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class J_equi_linear(pl.LightningModule):\n",
        "    def __init__(self, mv_dict_type, in_feat=5, out_feat=5, ):\n",
        "        super(J_equi_linear, self).__init__()\n",
        "\n",
        "        self.mv_dict_type = mv_dict_type # indicate if the multivector is expressed as 16 basis or they are all summed up\n",
        "        self.equi_linear_layer = torch.nn.Linear(in_features=in_feat, out_features=out_feat)\n",
        "\n",
        "    def forward(self, multivector):\n",
        "\n",
        "        if self.mv_dict_type == True: # first equivariant layer of the architecture\n",
        "            multivector_basis = list( multivector.keys() )\n",
        "\n",
        "            # summing up all scalars - vectorl - bivectorl - trivectorl - bias ( its '' basis )\n",
        "            bias = 0                     # should be: 1.0\n",
        "            vector_value = 0             # should be: 4.1\n",
        "            bivector_value = 0           # should be: 12.21\n",
        "            trivector_value = 0          # should be: 12.10\n",
        "            pseudoscalar_value = 0       # should be: 5.0\n",
        "\n",
        "            for basis in multivector_basis:\n",
        "                if len(basis) == 2:     vector_value += multivector[basis]\n",
        "                elif len(basis) == 3:   bivector_value += multivector[basis]\n",
        "                elif len(basis) == 4:   trivector_value += multivector[basis]\n",
        "                elif len(basis) == 5:   pseudoscalar_value += multivector[basis]\n",
        "                else:                   bias += multivector[basis]\n",
        "\n",
        "            # check correctness of position of each entries. Maybe switch bias and scalar value (?)\n",
        "            input_tensor = torch.tensor( [ bias, vector_value, bivector_value, trivector_value, pseudoscalar_value ] ,\n",
        "                                            dtype=torch.float32,\n",
        "                                            requires_grad=True\n",
        "                                        )\n",
        "\n",
        "            # check why print 12.2099999 instead of 12.1, can be a problem ?\n",
        "            print(f\"--Analizing input component:\\nbias(''): {bias}\\nvector(e_i): {vector_value}\\nbivector(e_ij): {bivector_value}\\ntrivector(e_ijk): {trivector_value}\\npseudoscalar(e_ijkl): {pseudoscalar_value}\")\n",
        "            print(f\"\\ninput_tensor: {input_tensor}\\n--input's analisys finished. \\n\\n\")\n",
        "        else:\n",
        "            input_tensor = multivector\n",
        "\n",
        "        output = self.equi_linear_layer( input_tensor )\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "my_layer = J_equi_linear(mv_dict_type=True)\n",
        "out = my_layer(dummy_mv)\n",
        "#print(out)"
      ],
      "metadata": {
        "id": "NhztcT6gVwm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class J_geo_attention(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(J_geo_attention, self).__init__()\n",
        "        #https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "        #https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
        "\n",
        "        tensor_length = 5\n",
        "        self.key   = nn.Parameter(torch.randn(tensor_length)).unsqueeze(0) # it is correct do it here? it is request to use 2 dimension\n",
        "        self.value = nn.Parameter(torch.randn(tensor_length)).unsqueeze(0)\n",
        "\n",
        "    def forward(self, multivector_query):\n",
        "        multivector_query = multivector_query.unsqueeze(0) # it is request to use 2 dimension\n",
        "        output = torch.nn.functional.scaled_dot_product_attention(multivector_query, self.key, self.value)\n",
        "\n",
        "        return output\n",
        "\n",
        "my_layer = J_geo_attention()"
      ],
      "metadata": {
        "id": "YAitQDS_58TI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class J_gatr(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # input layer: in = input\n",
        "        self.in_equi_linear_layer = J_equi_linear(in_feat=5, out_feat=5, mv_dict_type=True )\n",
        "\n",
        "        # first half block: h1 = first half\n",
        "        self.h1_norm_linear_layer   = J_norm_linear(normalized_shape=(5,))\n",
        "        self.h1_equi_linear_layer_1 = J_equi_linear(in_feat=5, out_feat=5, mv_dict_type=False )\n",
        "        self.h1_att_layer           = J_geo_attention()\n",
        "        self.h1_equi_linear_layer_2 = J_equi_linear(in_feat=5, out_feat=5, mv_dict_type=False )\n",
        "\n",
        "        # second half block: h2 = second half\n",
        "        self.h2_norm_linear_layer   = J_norm_linear(normalized_shape=(5,))\n",
        "        self.h2_equi_linear_layer_1 = J_equi_linear(in_feat=5, out_feat=5, mv_dict_type=False )\n",
        "        #self.h2_geo_bilinear_layer  =\n",
        "        #self.h2_gated_relu_layer    =\n",
        "        #self.h2_equi_linear_layer_2 = J_equi_linear(in_feat=?, out_feat=?, mv_dict_type=? )\n",
        "\n",
        "\n",
        "        # metrics\n",
        "        self.accuracy_metric  = BinaryAccuracy()\n",
        "        self.precision_metric = BinaryPrecision()\n",
        "        self.recall_metric    = BinaryRecall()\n",
        "        self.f1score_metric   = BinaryF1Score()\n",
        "\n",
        "\n",
        "    def forward(self, multivector):\n",
        "        do_print = True\n",
        "\n",
        "        print(f\"input: {multivector.values()}\\n\\nFirst block\")\n",
        "        # input layer\n",
        "        in_out_eq  = self.in_equi_linear_layer(multivector);       print(f\"in_out_eq: {in_out_eq}\")\n",
        "\n",
        "        # first half block\n",
        "        h1_out_norm = self.h1_norm_linear_layer( in_out_eq );        print(f\"h1_out_norm: {h1_out_norm}\")\n",
        "        h1_out_eq_1 = self.h1_equi_linear_layer_1( h1_out_norm );    print(f\"h1_out_eq_1: {h1_out_eq_1}\")\n",
        "        h1_out_att  = self.h1_att_layer( h1_out_eq_1 );              print(f\"h1_out_att : {h1_out_att}\")\n",
        "        h1_out_eq_2 = self.h1_equi_linear_layer_2( h1_out_att );     print(f\"h1_out_eq_2: {h1_out_eq_2}\")\n",
        "\n",
        "        # second half block\n",
        "        print(f\"\\n\\nSecond block\")\n",
        "        h2_out_norm       = self.h2_norm_linear_layer( h1_out_eq_2 + in_out_eq );   print(f\"h2_out_norm:       {h2_out_norm}\")\n",
        "        h2_out_eq_1       = self.h2_equi_linear_layer_1( h2_out_norm );             print(f\"h2_out_eq_1:       {h2_out_eq_1}\")\n",
        "        #h2_out_geo_bilin  = self.h2_geo_bilinear_layer( h2_out_eq_1 );              print(f\"h2_out_geo_bilin:  {h2_out_geo_bilin}\")\n",
        "        #h2_out_gated_relu = self.h2_gated_relu_layer( h2_out_geo_bilin );           print(f\"h2_out_fated_relu: {h2_out_gated_relu}\")\n",
        "        #h2_out_eq_2       = self.h2_equi_linear_layer_2( h2_out_gated_relu );       print(f\"h2_out_eq_2:       {h2_out_eq_2}\")\n",
        "\n",
        "\n",
        "\n",
        "        # output ( random for print)\n",
        "        output = h2_out_norm; print(f\"\\ngatr_output: {out}\")\n",
        "        #output = h2_out_eq2 + ( in_out_eq + h1_out_eq2 )\n",
        "\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        x, y = batch\n",
        "\n",
        "        # check if the used loss is correct according to the paper, find it on the github\n",
        "        loss = F.binary_cross_entropy(x, y) # In case we need to adjust dimension\n",
        "        #loss = F.binary_cross_entropy(self(x).view(-1), y.float())\n",
        "\n",
        "        #predictions = self.forward(x).long().squeeze()\n",
        "        #y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        #accuracy  = accuracy(predictions, y)\n",
        "        #precision = self.precision_metric(predictions, y)\n",
        "        #recall    = self.recall_metric(predictions, y)\n",
        "        #f1_score  = self.f1_metric(predictions, y)\n",
        "\n",
        "        #wandb.log({\"acc\": accuracy,\"loss\": loss,\"precision\": precision, \"recall\": recall, \"f1-score:\":f1_score })\n",
        "\n",
        "        return loss\n",
        "\n",
        "    '''\n",
        "    # look at https://colab.research.google.com/drive/1lUIrtEiQN9hA5RIdKTpUS7w5gI1DLvOl#scrollTo=gigZq4h0yifA\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return None\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return None\n",
        "    '''\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
        "\n",
        "gatr_j = J_gatr()"
      ],
      "metadata": {
        "id": "XYJ6UsYF0RqA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = gatr_j(dummy_mv)\n",
        "print(f\"\\noutput: {out}\")"
      ],
      "metadata": {
        "id": "ATUvJW7mqMH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5830b137-dffc-4c5c-ef4c-17d0123d7a8d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: dict_values([1.0, 1.01, 1.02, 1.03, 1.04, 2.01, 2.02, 2.03, 2.04, 2.05, 2.06, 3.01, 3.02, 3.03, 3.04, 5.0])\n",
            "\n",
            "First block\n",
            "--Analizing input component:\n",
            "bias(''): 1.0\n",
            "vector(e_i): 4.1000000000000005\n",
            "bivector(e_ij): 12.209999999999999\n",
            "trivector(e_ijk): 12.099999999999998\n",
            "pseudoscalar(e_ijkl): 5.0\n",
            "\n",
            "input_tensor: tensor([ 1.0000,  4.1000, 12.2100, 12.1000,  5.0000], requires_grad=True)\n",
            "--input's analisys finished. \n",
            "\n",
            "\n",
            "in_out_eq: tensor([-2.2162,  6.9824,  3.8340, -0.1660, -5.4338], grad_fn=<ViewBackward0>)\n",
            "h1_out_norm: tensor([-0.6423,  1.4557,  0.7376, -0.1747, -1.3762],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "h1_out_eq_1: tensor([ 0.3309,  0.3030,  0.2509, -0.5480, -0.4532], grad_fn=<ViewBackward0>)\n",
            "h1_out_att : tensor([[ 0.8517,  0.3165,  0.2526, -0.8711,  0.5608]], grad_fn=<MmBackward0>)\n",
            "h1_out_eq_2: tensor([[-0.5211,  0.4217,  0.6550, -0.8779,  0.1149]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "Second block\n",
            "h2_out_norm:       tensor([[-0.7018,  1.4576,  0.8369, -0.3412, -1.2515]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "h2_out_eq_1:       tensor([[-0.2828,  0.5229, -0.8258,  0.6378,  0.5463]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "gatr_output: tensor([[ 0.6990,  1.4770, -0.4729, -1.4243, -0.2788]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "output: tensor([[-0.7018,  1.4576,  0.8369, -0.3412, -1.2515]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Lorenzo's section\n",
        "---"
      ],
      "metadata": {
        "id": "3YDBpEMO0RVC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQJH2MT40U_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}